\section{Related Work and Preliminary Concepts}

\subsection{Infinite Multi-Armed-Bandit}

The infinite multi armed bandit problem consists of an infinite number of arms each with a quality parameter $\mu_i$ and every time they are pulled by an agent, they give a reward of $0$ or $1$ which is sampled from a Bernoulli distribution with the quality parameter as the mean. The agent that pulls the arm does not know the quality parameter of the arms before hand.

Every agent has a fixed number of rounds to play. At every round the agent is allowed to pull an arm that it has already pulled before or a new arm from the set of infinite arms. As each agent goes on pulling the arms, it goes on collecting rewards while simultaneously learning the qualities of each of the arms it has pulled.

\subsection{Regret}

\begin{definition}[Seen Regret]
\end{definition}

\subsection{UCB}
\subsection{Thompson Sampling}

\subsection{Fairness}